# BIV hack challenge

## Запуск
```
git clone https://github.com/AlexxxGorin/biv-hack.git
```
```
docker build -t turbo_bert_inference . 
```
```
docker run --rm -v $(pwd):/data turbo_bert_inference /data/payments_main.tsv /data/output.tsv
```

## Решение
### Данные
- Делубликация
- Генерация синтетики для обучения
- Ансамбль разметчик

### Аугментация
Дабы расширить нашу обучающая выборку, при этом сохранив распределение реальных данных, мы сгенерировали 15к дополнительнвх примеров с помощью  нескольких больших языковых моделей. То есть мы перефразировали текст, заменяли похожие слова, заменяли бренды и названия продуктов, при этом сохраняя формат реальных данных. Впоследствии это сильно помогло улучшить скор

### Разметка ансамблем 
Чтобы разметка наших 25 тысяч была максимально точной, мы сделали ансамбль разметки. В ансамбль вошли 3 общедоступных LLM,  которые на данный момент лучше всего работают с русским языком. Мы классифицировали каждый новый текст, подавая в качестве примера уже размеченные тексты, чтобы модели лучше справлялись. Также для повышения надежности обучили 2 трансформер модели, тоже предобученные специально для русского языка.

### Нормализация
Как дополнительный модуль, мы реализовали нормализацию текста. Проанализировав данные, мы заметили что многие тексты означающие одно и то же в датасете представлены по-разному. Мы же приводим их к единой форме с помощью LLM, а именно даты, аббревиатуры, и так дале.

### EDA
Также мы решали проблему несбалансированность датасета, взяв равномерное распределение и генерируя классы, которые представлены меньше других. Чтобы модель не скатилась в предсказание классов, которых большинство.

### Модель
- Наше решение устойчиво к изменению формата текста, так как архитектура нашей модели учитывает именно семантический смысл текста 
- Выдерживает огромную нагрузку, а именно при инференсе большого потка на один пример уходит приблизительно 12мс

Подробное описание обучения представлено в /training






